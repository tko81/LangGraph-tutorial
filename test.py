from typing import Annotated
from typing_extensions import TypedDict

from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain.chat_models import init_chat_model
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.types import Command, interrupt
from langchain_tavily import TavilySearch

"""
ğŸ” ä¸ºä»€ä¹ˆç¨‹åºæ‰§è¡Œå®Œå·¥å…·åå°±ç»“æŸäº†ï¼Ÿ

åŸå› åˆ†æï¼š
1. âŒ human_assistance å·¥å…·è°ƒç”¨äº† interrupt()ï¼Œè¿™ä¼šæš‚åœå›¾çš„æ‰§è¡Œ
2. âŒ interrupt() ä¼šæŠ›å‡º GraphInterrupt å¼‚å¸¸æ¥æš‚åœç¨‹åº
3. âŒ åŸä»£ç æ²¡æœ‰æ•è·è¿™ä¸ªå¼‚å¸¸ï¼Œæ‰€ä»¥ç¨‹åºç›´æ¥é€€å‡ºäº†
4. âŒ éœ€è¦æ£€æŸ¥å›¾çŠ¶æ€å¹¶ä½¿ç”¨ Command(resume=...) æ¥æ¢å¤æ‰§è¡Œ

è§£å†³æ–¹æ¡ˆï¼š
âœ… æ£€æŸ¥å›¾çŠ¶æ€ (get_state)
âœ… å‘ç°ä¸­æ–­åæä¾›äººç±»å›åº”
âœ… ä½¿ç”¨ Command æ¢å¤å›¾çš„æ‰§è¡Œ
"""

memory = InMemorySaver()

from dotenv import load_dotenv
load_dotenv()

# æ„å»ºstate schema
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# å°† human_assistance å·¥å…·æ·»åŠ åˆ° chatbot ä¸­ã€‚è¯¥å·¥å…·ä½¿ç”¨ interrupt æ¥æ”¶æ¥è‡ªäººç±»çš„ä¿¡æ¯
# ä¸ Python çš„å†…ç½® input() å‡½æ•°ç±»ä¼¼ï¼Œåœ¨å·¥å…·ä¸­è°ƒç”¨ interrupt å°†æš‚åœæ‰§è¡Œã€‚
# è¿›åº¦åŸºäºæ£€æŸ¥ç‚¹æŒä¹…åŒ–ã€‚å› æ­¤ï¼Œå¦‚æœå®ƒä½¿ç”¨ Postgres è¿›è¡ŒæŒä¹…åŒ–ï¼Œåªè¦æ•°æ®åº“å¤„äºæ´»åŠ¨çŠ¶æ€
# å®ƒå°±å¯ä»¥éšæ—¶æ¢å¤ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹è¿›è¡ŒæŒä¹…åŒ–ï¼Œåªè¦ Python å†…æ ¸æ­£åœ¨è¿è¡Œï¼Œå°±å¯ä»¥éšæ—¶æ¢å¤
@tool
def human_assistance(query: str) -> str:
    """
    Request assistance from a human. é‡åˆ°äººç±»æ±‚åŠ©æ—¶ï¼ŒLLMä¼šè°ƒç”¨è¿™ä¸ªå·¥å…·ï¼Œqueryæ˜¯æŒ‡æ‰“æ–­çš„é—®é¢˜æ˜¯ä»€ä¹ˆ
    """
    human_response = interrupt({"query": query})
    return human_response["data"]

# åˆå§‹åŒ–å·¥å…·
web_search = TavilySearch(max_results=2)
tools = [web_search, human_assistance]

# åˆå§‹åŒ–æ¨¡å‹ ç»‘å®šå·¥å…·
llm = init_chat_model("google_genai:gemini-2.0-flash")
llm_with_tools = llm.bind_tools(tools)

# å®šä¹‰Node
def chatbot(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}
# å¢åŠ Node
graph_builder.add_node("chatbot", chatbot)

# å®šä¹‰å·¥å…·Node
tool_node = ToolNode(tools)
# å¢åŠ å·¥å…·Node
graph_builder.add_node("my_tools", tool_node)

# å¢åŠ æ¡ä»¶è¾¹
graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,    # langgraphè‡ªå¸¦çš„å·¥å…·æ¡ä»¶å‡½æ•°è¿”å›å­—ç¬¦ä¸²æ˜¯å†™æ­»çš„ï¼Œå› æ­¤ä¸‹é¢çš„æ˜ å°„å­—å…¸ï¼Œåªèƒ½å†™toolså’Œ__end__
    {"tools": "my_tools", "__end__": END} # å¦‚æœä¸éœ€è¦å·¥å…·è°ƒç”¨ ç›´æ¥ç»“æŸ
)
# å¢åŠ æ™®é€šè¾¹
# Any time a tool is called, we return to the chatbot to decide the next step
graph_builder.add_edge("my_tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
#graph = graph_builder.compile()
# ä½¿ç”¨æä¾›çš„æ£€æŸ¥ç‚¹ç¼–è¯‘ graphï¼Œè¯¥æ£€æŸ¥ç‚¹å°†åœ¨ graph éå†æ¯ä¸ª Node æ—¶å°† State ä½œä¸ºæ£€æŸ¥ç‚¹
graph = graph_builder.compile(checkpointer=memory)


user_input = "I need some expert guidance for building an AI agent. Could you request assistance for me?"
config = {"configurable": {"thread_id": "1"}}

print("=== ç¬¬ä¸€é˜¶æ®µï¼šè¿è¡Œå›¾ç›´åˆ°ä¸­æ–­ ===")
try:
    events = graph.stream(
        {"messages": [{"role": "user", "content": user_input}]},
        config,
        stream_mode="values",
    )
    for event in events:
        if "messages" in event:
            event["messages"][-1].pretty_print()
except Exception as e:
    print(f"å›¾æ‰§è¡Œä¸­æ–­: {e}")

# æ£€æŸ¥å›¾çš„çŠ¶æ€
print("\n=== æ£€æŸ¥å›¾çŠ¶æ€ ===")
snapshot = graph.get_state(config)
print(f"ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹: {snapshot.next}")
print(f"ä¸­æ–­ä¿¡æ¯: {snapshot.interrupts}")

if snapshot.interrupts:
    print(f"ä¸­æ–­è¯¦æƒ…: {snapshot.interrupts[0].value}")
    
    # æ¨¡æ‹Ÿäººç±»å›åº”
    print("\n=== ç¬¬äºŒé˜¶æ®µï¼šäººç±»å›åº”å¹¶æ¢å¤æ‰§è¡Œ ===")
    human_response = (
        "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent."
        " It's much more reliable and extensible than simple autonomous agents."
    )
    
    print(f"äººç±»å›åº”: {human_response}")
    
    # åˆ›å»ºä¸€ä¸ª Command å¯¹è±¡ï¼ŒåŒ…å«æ¢å¤æ•°æ®human_response
    human_command = Command(resume={"data": human_response})
    # çœŸæ­£æ‰§è¡Œ Commandï¼Œå¯åŠ¨å›¾çš„æ¢å¤æµç¨‹ interrupt() è¿”å› {"data": human_response}
    # interrupt() è¿”å›åä¼šç»§ç»­æ‰§è¡Œå›¾ï¼Œå°†human_responseä½œä¸ºtool messageæ·»åŠ åˆ°stateçš„messageä¸­
    # æ¥ç€æµè½¬åˆ°ChatRobot Nodeï¼Œè¯»å–æœ€æ–°çš„æ¶ˆæ¯ï¼Œä¹Ÿå°±æ˜¯åˆšæ‰çš„tool messageåšå›å¤
    events = graph.stream(human_command, config, stream_mode="values")
    for event in events:
        if "messages" in event:
            event["messages"][-1].pretty_print()
else:
    print("æ²¡æœ‰å‘ç°ä¸­æ–­ï¼Œç¨‹åºæ­£å¸¸ç»“æŸ")
